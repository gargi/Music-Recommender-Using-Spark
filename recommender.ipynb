{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Recommender System using Apache Spark and Python\n",
    "\n",
    "## Description\n",
    "\n",
    "A recommender system that will recommend new musical artists to a user based on their listening history. Suggesting different songs or musical artists to a user is important to many music streaming services, such as Pandora and Spotify. In addition, this type of recommender system could also be used as a means of suggesting TV shows or movies to a user (e.g., Netflix). \n",
    "\n",
    "To create this system I have used Spark and collaborative filtering technique. \n",
    "\n",
    "## Datasets\n",
    "\n",
    "I have used publicly available song data from audioscrobbler, which can be found [here](http://www-etud.iro.umontreal.ca/~bergstrj/audioscrobbler_data.html). However, the original data files were modified so that the code will run in a reasonable time on a single machine. The reduced data files have been suffixed with `_small.txt` and contains only the information relevant to the top 50 most prolific users (highest artist play counts).\n",
    "\n",
    "The original data file `user_artist_data.txt` contained about 141,000 unique users, and 1.6 million unique artists. About 24.2 million usersâ€™ plays of artists are recorded, along with their count.\n",
    "\n",
    "Note that when plays are scribbled, the client application submits the name of the artist being played. This name could be misspelled or nonstandard, and this may only be detected later. For example, \"The Smiths\", \"Smiths, The\", and \"the smiths\" may appear as distinct artist IDs in the data set, even though they clearly refer to the same artist. So, the data set includes `artist_alias.txt`, which maps artist IDs that are known misspellings or variants to the canonical ID of that artist.\n",
    "\n",
    "The `artist_data.txt` file then provides a map from the canonical artist ID to the name of the artist."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary Package Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.recommendation import *\n",
    "import random\n",
    "from operator import *"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "\n",
    "Loads the three datasets into RDDs and name them `artistData`, `artistAlias`, and `userArtistData` and removes wrong IDs from userArtistData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def parseUserArtist(item):\n",
    "    item=item.split()\n",
    "    userId=int(item[0])\n",
    "    artistId=int(item[1])\n",
    "    count=int(item[2])\n",
    "    result_tup=(userId,artistId,count)\n",
    "    return result_tup\n",
    "def convertBadToGoodIds(item):\n",
    "    artistId=item[1]\n",
    "    if artistId in canonicalMap.keys():\n",
    "        artistId=canonicalMap.get(item[1])\n",
    "    return (item[0],artistId,item[2])  \n",
    "\n",
    "artistData = sc.textFile('artist_data_small.txt').map(lambda x: x.split('\\t')).map(lambda x: [int(x[0]), x[1]])\n",
    "artistAlias = sc.textFile('artist_alias_small.txt').map(lambda x: x.split('\\t')).map(lambda x: [int(x[0]), int(x[1])])\n",
    "canonicalMap = artistAlias.collectAsMap()\n",
    "userArtistData = sc.textFile('user_artist_data_small.txt').map(parseUserArtist)\n",
    "userArtistData=userArtistData.map(convertBadToGoodIds)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Finds the three users with the highest number of total play counts (sum of all counters) and prints the user ID, the total play count, and the mean play count (average number of times a user played an artist). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 1059637 has a total play count of 674412 and a mean play count of 1878.\n",
      "User 2064012 has a total play count of 548427 and a mean play count of 9455.\n",
      "User 2069337 has a total play count of 393515 and a mean play count of 1519.\n"
     ]
    }
   ],
   "source": [
    "splitUserData = userArtistData.map(lambda x: (x[0], x[2]))\n",
    "finalList = splitUserData.reduceByKey(lambda a,b: a + b).map(lambda x: (x[1], x[0])).sortByKey(False)\n",
    "countMap = splitUserData.countByKey()\n",
    "for item in finalList.collect()[0:3]:\n",
    "    print \"User %d has a total play count of %d and a mean play count of %d.\" %(item[1],item[0],(item[0]/countMap[item[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Splitting Data for Testing\n",
    "\n",
    "Divides the data (`userArtistData`) into:\n",
    "* A training set, `trainData`, that will be used to train the model which is 40% of the data.\n",
    "* A validation set, `validationData`, used to perform parameter tuning which is 40% of the data.\n",
    "* A test set, `testData`, used for a final evaluation of the model which is 20% of the data.\n",
    "\n",
    "A random seed value of 13 is used. Since these datasets will be repeatedly used it is persisted in memory using the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1059637, 1000049, 1), (1059637, 1000056, 1), (1059637, 1000113, 5)]\n",
      "[(1059637, 1000010, 238), (1059637, 1000062, 11), (1059637, 1000112, 423)]\n",
      "[(1059637, 1000094, 1), (1059637, 1000130, 19129), (1059637, 1000139, 4)]\n",
      "19817\n",
      "19633\n",
      "10031\n"
     ]
    }
   ],
   "source": [
    "trainData, validationData, testData = userArtistData.randomSplit([0.4, 0.4, 0.2], 13)\n",
    "trainData.cache()\n",
    "validationData.cache()\n",
    "testData.cache()\n",
    "print trainData.collect()[0:3]\n",
    "print validationData.collect()[0:3]\n",
    "print testData.collect()[0:3]\n",
    "print trainData.count()\n",
    "print validationData.count()\n",
    "print testData.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Recommender Model\n",
    "\n",
    "The model is trained with implicit feedback. You can read more information about this from the collaborative filtering page: [http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html](http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html). The function (http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.recommendation.ALS.trainImplicit) has a few tunable parameters that will affect how the model is built. Therefore, to get the best model, a small parameter sweep is done to choose the model that performs the best on the validation set.\n",
    "\n",
    "After building a model for evaluation, a parameter sweep is done, evaluating each combination of parameters on the validation data to choose the optimal set of parameters. The parameters are then used to make predictions on the test data.\n",
    "\n",
    "### Model Evaluation\n",
    "\n",
    "Suppose we have a model and some dataset of *true* artist plays for a set of users. This model can be used to predict the top X artist recommendations for a user and these recommendations can be compared the artists that the user actually listened to (here, X will be the number of artists in the dataset of *true* artist plays). Then, the fraction of overlap between the top X predictions of the model and the X artists that the user actually listened to can be calculated. This process is repeated for all users and an average value returned.\n",
    "\n",
    "For example, suppose a model predicted [1,2,4,8] as the top X=4 artists for a user. Suppose, that user actually listened to the artists [1,3,7,8]. Then, for this user, the model would have a score of 2/4=0.5. To get the overall score, this would be performed for all users, with the average returned.\n",
    "\n",
    "**NOTE: when using the model to predict the top-X artists for a user, the artists listed with that user in the  training data are not used.**\n",
    "\n",
    "`modelEval` takes a model (the output of ALS.trainImplicit) and a dataset as input. For parameter tuning, the dataset parameter is set to the validation data (`validationData`). After parameter tuning, the model is evaluated on the test data (`testData`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def modelEval(model,dataset):\n",
    "    \n",
    "    subDataset=dataset.map(lambda x:(x[0],x[1])).groupByKey()\n",
    "    datasetMap = subDataset.collectAsMap()\n",
    "    subTrainData=trainData.map(lambda x:(x[0],x[1])).groupByKey()\n",
    "    trainDataMap = subTrainData.collectAsMap()\n",
    "    allArtists=artistData.map(lambda x:(x[0]))\n",
    "    allArtists = allArtists.collect()\n",
    "    total=0.0\n",
    "    userCount=0\n",
    "    \n",
    "    for user in datasetMap.keys():\n",
    "        artistsInTrainData=trainDataMap.get(user)\n",
    "        artistsNotInTrainData=[]\n",
    "        for x in allArtists:\n",
    "            if x not in artistsInTrainData:\n",
    "                artistsNotInTrainData.append(x)\n",
    "        result=[]\n",
    "        for x in artistsNotInTrainData:\n",
    "            record=(user,x)\n",
    "            result.append(record)\n",
    "        finalRDD=sc.parallelize(result)\n",
    "        trueArtists=datasetMap.get(user) \n",
    "        X=len(trueArtists)\n",
    "        finalResult=model.predictAll(finalRDD)\n",
    "        prediction = finalResult.map(lambda x: (x[2], x[1])).sortByKey(False).map(lambda x: x[1])\n",
    "        total += len(set(prediction.take(X)).intersection(set(trueArtists)))/float(X)\n",
    "        userCount=userCount+1\n",
    "        \n",
    "    \n",
    "    print \"The model score for rank %d is %f\"%(rank,float(total/float(userCount)))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Construction\n",
    "\n",
    "The best model is built using the validation set of data and the `modelEval` function. Although, there are a few parameters we could optimize, for the sake of time, I have tried a few different values for the [rank parameter](http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html#collaborative-filtering) (everything else is left at its default value, **except make `seed`=345**). Loop through the values [2, 10, 20] and figure out which one produces the highest scored based on your model evaluation function.\n",
    "\n",
    "Note: this procedure may take several minutes to run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model score for rank 2 is 0.093266\n",
      "The model score for rank 10 is 0.097496\n",
      "The model score for rank 20 is 0.083883\n"
     ]
    }
   ],
   "source": [
    "ranks=[2,10,20]\n",
    "for rank in ranks:\n",
    "    Model = ALS.trainImplicit(trainData, rank=rank, seed=345)\n",
    "    rank = rank\n",
    "    modelEval(Model, validationData)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "bestModel = ALS.trainImplicit(trainData, rank=10, seed=345)\n",
    "modelEval(bestModel, testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top X recommended Artists for a particular user can be computer as below using the Best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artist 0: blink-182\n",
      "Artist 1: Elliott Smith\n",
      "Artist 2: Taking Back Sunday\n",
      "Artist 3: Incubus\n",
      "Artist 4: Death Cab for Cutie\n"
     ]
    }
   ],
   "source": [
    "topFive = bestModel.recommendProducts(1059637,5)\n",
    "artistMap=artistData.collectAsMap()\n",
    "i = 0\n",
    "for artist in topFive:\n",
    "    print \"Artist \" + str(i) + \": \" + artistMap.get(artist[1])\n",
    "    i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
